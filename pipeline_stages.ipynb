{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Pipeline Stages: T-gate Cultivation Sampler\n",
    "\n",
    "This notebook walks through each stage of the compilation pipeline,\n",
    "printing intermediate structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re, time\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "_THIS_DIR = Path('.').resolve()\n",
    "sys.path.insert(0, str(_THIS_DIR))\n",
    "\n",
    "import tsim, stim, numpy as np\n",
    "import pyzx_param as zx\n",
    "import gen\n",
    "\n",
    "from d_3_circuit_definitions import circuit_source_injection_T\n",
    "\n",
    "def replace_t_with_s(s):\n",
    "    s = re.sub(r'^(\\s*)T_DAG(\\s)', r'\\1S_DAG\\2', s, flags=re.MULTILINE)\n",
    "    return re.sub(r'^(\\s*)T(\\s)', r'\\1S\\2', s, flags=re.MULTILINE)\n",
    "\n",
    "def replace_s_with_t(c):\n",
    "    p = str(c)\n",
    "    p = re.sub(r'^(\\s*)S_DAG(\\s)', r'\\1T_DAG\\2', p, flags=re.MULTILINE)\n",
    "    return re.sub(r'^(\\s*)S(\\s)', r'\\1T\\2', p, flags=re.MULTILINE)\n",
    "\n",
    "NOISE_STRENGTH = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage0-header",
   "metadata": {},
   "source": [
    "## Stage 0: Raw Stim Circuit\n",
    "\n",
    "The starting point is a stim circuit defining the distance-3 T state cultivation\n",
    "protocol. We add depolarising noise and convert S gates back to T gates for\n",
    "non-Clifford simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stage0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the projection + observable\n",
    "_projection_T_Proj_decode = \"\\nTICK\\nCX 8 3\\nCX 11 6\\nCX 0 9\\nTICK\\nCX 8 14\\nCX 11 9\\nCX 0 3\\nTICK\\nCX 11 14\\nCX 8 9\\nCX 0 6\\nTICK\\nCX 6 14\\nTICK\\nCX 6 3\\nTICK\\n\"\n",
    "_projection_T_Proj_measure = \"\\nTICK\\nMX 0 11 8\\nM 9 3 14\\nMX 6\\nDETECTOR(0.625, 0.125, 0, -1, -9) rec[-20] rec[-19] rec[-14] rec[-7]\\nDETECTOR(0.875, 0.125, 0, -1, -9) rec[-17] rec[-4]\\nDETECTOR(1.25, 1.4375, 0, -1, -9) rec[-20] rec[-14] rec[-6] rec[-5]\\nDETECTOR(1.5, 1.4375, 0, -1, -9) rec[-16] rec[-3]\\nDETECTOR(2.5, 0.9375, 0, -1, -9) rec[-14] rec[-6]\\nDETECTOR(2.75, 0.9375, 0, -1, -9) rec[-15] rec[-2]\\n\"\n",
    "\n",
    "# Clifford version of injection circuit\n",
    "clifford_source = replace_t_with_s(circuit_source_injection_T)\n",
    "\n",
    "# Find observable include\n",
    "from run import compute_projection_obs_include\n",
    "proj_str_t, noiseless_raw = compute_projection_obs_include(clifford_source)\n",
    "\n",
    "# Build noisy circuit\n",
    "clifford_circuit = stim.Circuit(clifford_source)\n",
    "noise_model = gen.NoiseModel.uniform_depolarizing(NOISE_STRENGTH)\n",
    "noisy_clifford = noise_model.noisy_circuit_skipping_mpp_boundaries(clifford_circuit)\n",
    "noisy_injection_str = replace_s_with_t(noisy_clifford)\n",
    "\n",
    "c_injection = tsim.Circuit(noisy_injection_str)\n",
    "c_projection = tsim.Circuit(proj_str_t)\n",
    "circ = c_injection + c_projection\n",
    "\n",
    "# Use Clifford version for stim stats (stim can't parse T gates)\n",
    "proj_clifford = _projection_T_Proj_decode + \"S 6\\n\" + _projection_T_Proj_measure\n",
    "stim_circ = stim.Circuit(str(noisy_clifford) + proj_clifford + \"OBSERVABLE_INCLUDE(0) rec[-1]\\n\")\n",
    "\n",
    "print('=== Stage 0: Raw Circuit ===')\n",
    "print(f'  Qubits:       {stim_circ.num_qubits}')\n",
    "print(f'  Measurements: {stim_circ.num_measurements}')\n",
    "print(f'  Detectors:    {stim_circ.num_detectors}')\n",
    "print(f'  Observables:  {stim_circ.num_observables}')\n",
    "print(f'  Noiseless observable raw: {noiseless_raw}')\n",
    "print()\n",
    "print('First 20 lines of noisy injection circuit (T-gate version):')\n",
    "lines = noisy_injection_str.strip().split('\\n')\n",
    "for line in lines[:20]:\n",
    "    print(f'  {line}')\n",
    "print(f'  ... ({len(lines)} lines total)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage1-header",
   "metadata": {},
   "source": [
    "## Stage 1: Graph Preparation (ZX Calculus)\n",
    "\n",
    "`prepare_graph()` converts the tsim Circuit into a ZX-calculus graph representation,\n",
    "extracting boundary vertices, parameter names (f-params for noise, m-params for\n",
    "measurements), and error channel information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stage1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsim.core.graph import prepare_graph, get_params, connected_components\n",
    "\n",
    "prepared = prepare_graph(circ, sample_detectors=True)\n",
    "graph = prepared.graph\n",
    "\n",
    "print('=== Stage 1: Prepared ZX Graph ===')\n",
    "print(f'  Vertices:      {graph.num_vertices()}')\n",
    "print(f'  Edges:         {graph.num_edges()}')\n",
    "print(f'  Inputs:        {len(graph.inputs())}')\n",
    "print(f'  Outputs:       {len(graph.outputs())}')\n",
    "print(f'  Num outputs:   {prepared.num_outputs}')\n",
    "print(f'  Num detectors: {prepared.num_detectors}')\n",
    "print()\n",
    "\n",
    "# Parameters\n",
    "all_params = get_params(graph)\n",
    "f_params = sorted([p for p in all_params if p.startswith('f')])\n",
    "m_params = sorted([p for p in all_params if p.startswith('m')])\n",
    "print(f'  f-params (noise):       {len(f_params)}')\n",
    "print(f'  m-params (measurement): {len(m_params)}')\n",
    "print(f'  First 10 f-params: {f_params[:10]}')\n",
    "print(f'  m-params: {m_params}')\n",
    "print()\n",
    "\n",
    "# Channel info\n",
    "from collections import Counter\n",
    "n_channels = len(prepared.channel_probs)\n",
    "outcome_counts = [len(cp) for cp in prepared.channel_probs]\n",
    "outcome_dist = Counter(outcome_counts)\n",
    "print(f'  Channels: {n_channels}')\n",
    "print(f'  Outcomes per channel: {dict(sorted(outcome_dist.items()))}')\n",
    "print(f'  Error transform entries: {len(prepared.error_transform)}')\n",
    "\n",
    "# Vertex type distribution\n",
    "type_names = {0: 'boundary', 1: 'Z-spider', 2: 'X-spider'}\n",
    "type_counts = Counter(graph.type(v) for v in graph.vertices())\n",
    "print(f'\\n  Vertex types:')\n",
    "for t, count in sorted(type_counts.items()):\n",
    "    print(f'    {type_names.get(t, f\"type-{t}\")}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "slwgjhz9caa",
   "metadata": {},
   "source": [
    "### Full ZX Graph (Stage 1)\n",
    "The complete prepared graph â€” likely large. Green = Z-spiders, red = X-spiders, grey = boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1qp8eus7v6h",
   "metadata": {},
   "outputs": [],
   "source": [
    "zx.draw(graph, figsize=(18, 10), labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage2-header",
   "metadata": {},
   "source": [
    "## Stage 2: Connected Components\n",
    "\n",
    "The ZX graph decomposes into independent connected components. Each component\n",
    "maps to a subset of output indices (detectors + observables). Components are\n",
    "compiled independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stage2",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = connected_components(prepared.graph)\n",
    "sorted_components = sorted(components, key=lambda c: len(c.output_indices))\n",
    "\n",
    "print(f'=== Stage 2: Connected Components ===')\n",
    "print(f'  Total components: {len(components)}')\n",
    "print()\n",
    "\n",
    "for i, cc in enumerate(sorted_components):\n",
    "    g = cc.graph\n",
    "    n_out = len(g.outputs())\n",
    "    params = get_params(g)\n",
    "    f_p = [p for p in params if p.startswith('f')]\n",
    "    m_p = [p for p in params if p.startswith('m')]\n",
    "    \n",
    "    # Count vertex types\n",
    "    tc = Counter(g.type(v) for v in g.vertices())\n",
    "    \n",
    "    # Count T-like phases\n",
    "    from stab_rank_cut import is_t_like\n",
    "    t_count = sum(1 for v in g.vertices() if is_t_like(g.phase(v)))\n",
    "    \n",
    "    print(f'  Component {i}: {g.num_vertices()} vertices, '\n",
    "          f'{g.num_edges()} edges, {n_out} outputs')\n",
    "    print(f'    Output indices: {cc.output_indices}')\n",
    "    print(f'    f-params: {len(f_p)}, m-params: {len(m_p)}')\n",
    "    print(f'    T-count: {t_count}')\n",
    "    print(f'    Z-spiders: {tc.get(1, 0)}, X-spiders: {tc.get(2, 0)}, '\n",
    "          f'boundary: {tc.get(0, 0)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yl0f2h3e9ub",
   "metadata": {},
   "source": [
    "### Connected Component Graphs (Stage 2)\n",
    "Draw the smallest and largest components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n6q4j9t6yn8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smallest component\n",
    "print(f'Smallest component (component 0): {sorted_components[0].graph.num_vertices()} vertices, '\n",
    "      f'{len(sorted_components[0].output_indices)} outputs')\n",
    "zx.draw(sorted_components[0].graph, figsize=(10, 5), labels=True)\n",
    "\n",
    "# Largest component\n",
    "print(f'\\nLargest component (component {len(sorted_components)-1}): '\n",
    "      f'{sorted_components[-1].graph.num_vertices()} vertices, '\n",
    "      f'{len(sorted_components[-1].output_indices)} outputs')\n",
    "zx.draw(sorted_components[-1].graph, figsize=(18, 10), labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage3-header",
   "metadata": {},
   "source": [
    "## Stage 3: Cutting Decomposition\n",
    "\n",
    "For each component, non-Clifford (T-gate) spiders are decomposed via the\n",
    "cutting rule: each T-gate is split into 2 branches, doubling the term count.\n",
    "After cutting, `full_reduce` simplifies, and remaining T-gates use BSS.\n",
    "\n",
    "We demonstrate on the largest component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stage3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stab_rank_cut import decompose as stab_rank_decompose\n",
    "\n",
    "# Pick the largest component (most outputs)\n",
    "demo_cc = sorted_components[-1]\n",
    "demo_g = deepcopy(demo_cc.graph)\n",
    "\n",
    "print(f'=== Stage 3: Cutting Decomposition ===')\n",
    "print(f'  Demonstrating on component with {len(demo_cc.output_indices)} outputs')\n",
    "print(f'  Initial: {demo_g.num_vertices()} vertices, {demo_g.num_edges()} edges')\n",
    "print()\n",
    "\n",
    "# Step-by-step cutting with debug\n",
    "g_work = deepcopy(demo_g)\n",
    "zx.full_reduce(g_work, paramSafe=True)\n",
    "tc = zx.simplify.tcount(g_work)\n",
    "print(f'  After full_reduce: {g_work.num_vertices()} vertices, '\n",
    "      f'{g_work.num_edges()} edges, T-count={tc}')\n",
    "print()\n",
    "\n",
    "# Show cutting iterations\n",
    "print('  Cutting iterations (on a fresh copy):')\n",
    "cut_terms = stab_rank_decompose(\n",
    "    deepcopy(demo_g),\n",
    "    debug=True,\n",
    "    use_bss_fallback=False,\n",
    "    max_iterations=10,\n",
    "    param_safe=True,\n",
    "    cut_strategy='fewest_neighbors',\n",
    "    use_tsim_bss=False,\n",
    ")\n",
    "print(f'\\n  Cutting produced {len(cut_terms)} terms')\n",
    "\n",
    "# Reduce each and show stats\n",
    "clifford_count = 0\n",
    "remaining_t = 0\n",
    "for i, term in enumerate(cut_terms):\n",
    "    zx.full_reduce(term, paramSafe=True)\n",
    "    tc = zx.simplify.tcount(term)\n",
    "    if tc == 0:\n",
    "        clifford_count += 1\n",
    "    else:\n",
    "        remaining_t += tc\n",
    "\n",
    "print(f'  After reducing cut terms: {clifford_count} Clifford, '\n",
    "      f'{len(cut_terms) - clifford_count} with remaining T-count={remaining_t}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llfcwaxzah",
   "metadata": {},
   "source": [
    "### Cutting Decomposition Graphs (Stage 3)\n",
    "The reduced graph before cutting, and a sample of the resulting Clifford terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ehgnuutuymh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced graph before cutting (T-gates still present)\n",
    "print(f'Reduced graph (before cutting): T-count={zx.simplify.tcount(g_work)}')\n",
    "zx.draw(g_work, figsize=(14, 8), labels=True)\n",
    "\n",
    "# First few Clifford terms after cutting + reduce\n",
    "n_show = min(3, len(cut_terms))\n",
    "for i in range(n_show):\n",
    "    tc_i = zx.simplify.tcount(cut_terms[i])\n",
    "    print(f'\\nCut term {i}: {cut_terms[i].num_vertices()} vertices, T-count={tc_i}, '\n",
    "          f'scalar.power2={cut_terms[i].scalar.power2}')\n",
    "    zx.draw(cut_terms[i], figsize=(10, 5), labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage4-header",
   "metadata": {},
   "source": [
    "## Stage 4: Output Plugging & Disconnection\n",
    "\n",
    "For enumeration-based sampling, outputs are plugged (set to 0-effect with\n",
    "m-parameter phase). After `full_reduce`, the fully-plugged graph may disconnect\n",
    "into independent sub-components, enabling product evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stage4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsim_cutting import _get_f_indices, _plug_outputs, _find_zx_components\n",
    "\n",
    "# Use same demo component\n",
    "demo_g2 = deepcopy(demo_cc.graph)\n",
    "output_indices = demo_cc.output_indices\n",
    "num_outputs = len(demo_g2.outputs())\n",
    "\n",
    "f_indices_global = _get_f_indices(prepared.graph)\n",
    "component_f_set = set(_get_f_indices(demo_g2))\n",
    "f_selection = [i for i in f_indices_global if i in component_f_set]\n",
    "component_m_chars = [f'm{i}' for i in output_indices]\n",
    "\n",
    "print(f'=== Stage 4: Output Plugging ===')\n",
    "print(f'  Component: {num_outputs} outputs, output_indices={output_indices}')\n",
    "print(f'  f-params used: {len(f_selection)}')\n",
    "print()\n",
    "\n",
    "# Plug: level-0 (normalization) and fully-plugged\n",
    "plugged_graphs = _plug_outputs(demo_g2, component_m_chars, [0, num_outputs])\n",
    "\n",
    "# Level 0: normalization\n",
    "g_level0 = deepcopy(plugged_graphs[0])\n",
    "print(f'  Level-0 (norm, no outputs plugged):')\n",
    "print(f'    Before reduce: {g_level0.num_vertices()} vertices, '\n",
    "      f'{g_level0.num_edges()} edges')\n",
    "zx.full_reduce(g_level0, paramSafe=True)\n",
    "g_level0.normalize()\n",
    "tc0 = zx.simplify.tcount(g_level0)\n",
    "print(f'    After reduce:  {g_level0.num_vertices()} vertices, '\n",
    "      f'{g_level0.num_edges()} edges, T-count={tc0}')\n",
    "print(f'    power2_base:   {g_level0.scalar.power2}')\n",
    "print()\n",
    "\n",
    "# Fully plugged\n",
    "g_plugged = deepcopy(plugged_graphs[1])\n",
    "print(f'  Fully-plugged (all {num_outputs} outputs plugged):')\n",
    "print(f'    Before reduce: {g_plugged.num_vertices()} vertices, '\n",
    "      f'{g_plugged.num_edges()} edges')\n",
    "zx.full_reduce(g_plugged, paramSafe=True)\n",
    "g_plugged.normalize()\n",
    "tc_full = zx.simplify.tcount(g_plugged)\n",
    "print(f'    After reduce:  {g_plugged.num_vertices()} vertices, '\n",
    "      f'{g_plugged.num_edges()} edges, T-count={tc_full}')\n",
    "print()\n",
    "\n",
    "# Check disconnection\n",
    "zx_comps = _find_zx_components(g_plugged)\n",
    "print(f'  Disconnection check:')\n",
    "if len(zx_comps) >= 2:\n",
    "    print(f'    Graph DISCONNECTS into {len(zx_comps)} sub-components!')\n",
    "    for j, comp_verts in enumerate(zx_comps):\n",
    "        comp_params = set()\n",
    "        for v in comp_verts:\n",
    "            phase = g_plugged.phase(v)\n",
    "            if isinstance(phase, str):\n",
    "                comp_params.add(phase)\n",
    "        print(f'    Sub-component {j}: {len(comp_verts)} vertices, '\n",
    "              f'params: {sorted(comp_params)[:5]}...' if len(comp_params) > 5 else\n",
    "              f'    Sub-component {j}: {len(comp_verts)} vertices, '\n",
    "              f'params: {sorted(comp_params)}')\n",
    "else:\n",
    "    print(f'    Graph does NOT disconnect (monolithic evaluation)')\n",
    "    print(f'    Single component: {len(zx_comps[0])} vertices')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4gu7qpy4v",
   "metadata": {},
   "source": [
    "### Plugged Graph & Sub-components (Stage 4)\n",
    "The fully-plugged reduced graph, and its disconnected sub-components (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sqmtg788xb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsim_cutting import _extract_subgraph\n",
    "\n",
    "# Draw the fully-plugged reduced graph\n",
    "print(f'Fully-plugged graph (reduced): {g_plugged.num_vertices()} vertices, '\n",
    "      f'T-count={zx.simplify.tcount(g_plugged)}')\n",
    "zx.draw(g_plugged, figsize=(14, 8), labels=True)\n",
    "\n",
    "# Draw sub-components if disconnected\n",
    "if len(zx_comps) >= 2:\n",
    "    for j, comp_verts in enumerate(zx_comps):\n",
    "        sub_g = _extract_subgraph(g_plugged, comp_verts, reset_scalar=(j > 0))\n",
    "        print(f'\\nSub-component {j}: {sub_g.num_vertices()} vertices')\n",
    "        zx.draw(sub_g, figsize=(10, 5), labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage5-header",
   "metadata": {},
   "source": [
    "## Stage 5: Compiled Program\n",
    "\n",
    "The full compilation produces a `SubcompEnumCompiledProgram` with per-component\n",
    "data. Each component stores compiled scalar graphs (A/B/C/D terms), combo\n",
    "tables, and parameter index maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stage5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsim_cutting import compile_program_subcomp_enum_general\n",
    "from tsim_cutting import SubcompEnumComponentData, SubcompComponentData\n",
    "\n",
    "t0 = time.time()\n",
    "program = compile_program_subcomp_enum_general(\n",
    "    prepared, max_cut_iterations=10, debug=False\n",
    ")\n",
    "compile_time = time.time() - t0\n",
    "\n",
    "print(f'=== Stage 5: Compiled Program ===')\n",
    "print(f'  Compilation time: {compile_time:.2f}s')\n",
    "print(f'  Total components: {len(program.component_data)}')\n",
    "print(f'  Total outputs:    {program.num_outputs}')\n",
    "print(f'  Total f-params:   {program.num_f_params}')\n",
    "print(f'  Num detectors:    {program.num_detectors}')\n",
    "print()\n",
    "\n",
    "n_enum = 0\n",
    "n_subcomp = 0\n",
    "total_d_terms = 0\n",
    "\n",
    "for i, cd in enumerate(program.component_data):\n",
    "    if isinstance(cd, SubcompEnumComponentData):\n",
    "        n_enum += 1\n",
    "        n_out = cd.num_component_outputs\n",
    "        n_combos = cd.m_combos.shape[0]\n",
    "        n_sub = cd.num_subcomps\n",
    "        \n",
    "        d_terms = 0\n",
    "        for sc in cd.subcomp_compiled:\n",
    "            d_terms += sc.d_const_alpha.shape[0] * sc.d_const_alpha.shape[1]\n",
    "        total_d_terms += d_terms\n",
    "        \n",
    "        print(f'  Component {i} [ENUM]: {n_out} outputs, '\n",
    "              f'{n_combos} combos, {n_sub} sub-comp(s), '\n",
    "              f'{d_terms} D-terms')\n",
    "        print(f'    output_indices: {cd.output_indices}')\n",
    "        print(f'    f_selection: {len(cd.f_selection)} params')\n",
    "        for j, sc in enumerate(cd.subcomp_compiled):\n",
    "            n_graphs = sc.phase_indices.shape[0]\n",
    "            print(f'    Sub-comp {j}: {n_graphs} scalar graphs, '\n",
    "                  f'A-terms={sc.a_const_phases.shape}, '\n",
    "                  f'D-terms={sc.d_const_alpha.shape}')\n",
    "    else:\n",
    "        n_subcomp += 1\n",
    "        n_out = len(cd.output_indices)\n",
    "        n_levels = len(cd.compiled_scalar_graphs)\n",
    "        has_prod = cd.has_product_level\n",
    "        \n",
    "        d_terms = 0\n",
    "        for csg in cd.compiled_scalar_graphs:\n",
    "            d_terms += csg.d_const_alpha.shape[0] * csg.d_const_alpha.shape[1]\n",
    "        total_d_terms += d_terms\n",
    "        \n",
    "        print(f'  Component {i} [AUTOREGRESSIVE]: {n_out} outputs, '\n",
    "              f'{n_levels} levels, product={has_prod}, '\n",
    "              f'{d_terms} D-terms')\n",
    "        print(f'    output_indices: {cd.output_indices}')\n",
    "\n",
    "print(f'\\n  Summary: {n_enum} enum + {n_subcomp} autoregressive components')\n",
    "print(f'  Total D-terms: {total_d_terms}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage6-header",
   "metadata": {},
   "source": [
    "## Stage 6: Sampler + Optimizations\n",
    "\n",
    "The final sampler wraps the compiled program with a channel sampler for noise.\n",
    "We then apply the acceleration patches:\n",
    "1. Noiseless cache (skip evaluation for identity-channel shots)\n",
    "2. Dedup (numpy hash-based deduplication of f-params)\n",
    "3. Inverse CDF channel sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stage6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsim_cutting import compile_detector_sampler_subcomp_enum_general\n",
    "from evaluate_matmul_cfloat import evaluate_batch as evaluate_batch_cfloat\n",
    "import tsim_cutting as mod_cutting\n",
    "mod_cutting.evaluate_batch = evaluate_batch_cfloat\n",
    "\n",
    "from sampler_noiseless_cache import add_noiseless_cache\n",
    "from sampler_dedup import patch_sampler_fast\n",
    "from channel_sampler_fast import patch_channel_sampler_fast\n",
    "\n",
    "print('=== Stage 6: Sampler Assembly ===')\n",
    "print()\n",
    "\n",
    "# Base sampler\n",
    "sampler = compile_detector_sampler_subcomp_enum_general(circ, seed=42, max_cut_iterations=10)\n",
    "cs = sampler._channel_sampler\n",
    "print(f'Channel sampler:')\n",
    "print(f'  Channels:    {len(cs.channels)}')\n",
    "print(f'  Outcomes per channel: {[len(ch.logits) for ch in cs.channels[:5]]}... ')\n",
    "print()\n",
    "\n",
    "# Patch 1: Noiseless cache\n",
    "add_noiseless_cache(sampler)\n",
    "print()\n",
    "\n",
    "# Patch 2: Dedup\n",
    "patch_sampler_fast(sampler, top_k=None, max_unique=None, use_dedup=True, verbose=True)\n",
    "print()\n",
    "\n",
    "# Patch 3: Inverse CDF channel sampling\n",
    "patch_channel_sampler_fast(sampler, verbose=True)\n",
    "print()\n",
    "\n",
    "# Quick test\n",
    "print('Quick sample test (1024 shots)...')\n",
    "det, obs = sampler.sample(shots=1024, batch_size=1024, separate_observables=True)\n",
    "trivial = np.all(det == 0, axis=1)\n",
    "n_kept = int(np.sum(trivial))\n",
    "n_errors = int(np.sum(obs[trivial, 0].astype(int) != noiseless_raw))\n",
    "print(f'  Kept: {n_kept}/1024 (PSR={n_kept/1024:.3f})')\n",
    "print(f'  Errors: {n_errors}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
